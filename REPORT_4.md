# Домашнее задание к уроку 4: Сверточные сети

## Цель задания
Сравнить эффективность сверточных и полносвязных сетей на задачах компьютерного зрения, изучить преимущества CNN архитектур.
### 1.1 Сравнение на MNIST (20 баллов)
```python
# Сравните производительность на MNIST:
# - Полносвязная сеть (3-4 слоя)
# - Простая CNN (2-3 conv слоя)
# - CNN с Residual Block
# 
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность на train и test множествах
# - Измерьте время обучения и инференса
# - Визуализируйте кривые обучения
# - Проанализируйте количество параметров
```
### Создал три модели по заданию FCN (3-4 слоя), простую CNN (2-3 conv слоя) и CNN с Residual блоком с одинаковыми гиперпараметрами (15 эпох, lr = 1e-3).
### Загрузил MNIST и CIFAR датасеты для тестирования разных моделей. Написал единую функцию для обучения и оценки моделей с optimizer Adam и criterion CrossEntropyLoss. Также функция сохраняет loss и acc на каждой эпохе для визуалазации после обучения.
### Графики Loss и Acuracy для 1.1 MNIST
![1.1_MNIST](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/1.1_MNIST.png)
### Выводы из графиков для MNIST
1. Модели показывают примерно одинаковые accuracy > 0.98 . После 3-4 эпохи accuracy перестает стабильно расти
2. Лучшая модель по accuracy = 0.9946 и с самым низким loss RESNETCNN. Она же дольше всех обучается, но имеет меньше всего параметров
### 1.2 Сравнение на CIFAR-10 (20 баллов)
```python

# Сравните производительность на CIFAR-10:
# - Полносвязная сеть (глубокая)
# - CNN с Residual блоками
# - CNN с регуляризацией и Residual блоками
# 
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность и время обучения
# - Проанализируйте переобучение
# - Визуализируйте confusion matrix
# - Исследуйте градиенты (gradient flow)
```
###  FCN и CNNResNet использовал те же, что и в 1.1. Написал класс для Residual блок 2 с регуляризацией (Batchnorm + Dropout) для CNNResRegNet.
### Для CNNResNet и CNNResRegNet использовал 25 эпох, чтобы точнее определить, что лучше
### Графики Loss и Acuracy для 1.2 CIFAR
![1.2_CIFAR](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/1.2_Cifar.png)
### Как видно из графиков accuracy у RESNET с регуляризацией выше всех 0.8322. Немного хуже обычный RESNET (хотя до 15 эпохи он был лучше). При большем количестве эпох RESREGNET скорее всего еще немного увеличит отрыв от RESNET
### Выведем confusion матрицы для каждой модели
![1.2_Conf_matrixes_1](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/1.2_Conf_matrixes_1.png)
![1.2_Conf_matrixes_2](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/1.2_Conf_matrixes_2.png)
### Confusion матрицы подтверждают предыдущие результаты accuracy
### Задание 2: Анализ архитектур CNN (30 баллов) Создайте файл homework_cnn_architecture_analysis.py:

2.1 Влияние размера ядра свертки (15 баллов)
```python
# Исследуйте влияние размера ядра свертки:
# - 3x3 ядра
# - 5x5 ядра
# - 7x7 ядра
# - Комбинация разных размеров (1x1 + 3x3)
# 
# Для каждого варианта:
# - Поддерживайте одинаковое количество параметров
# - Сравните точность и время обучения
# - Проанализируйте рецептивные поля
# - Визуализируйте активации первого слоя
```
### Написал новый Residual class с padding = kernel_size // 2 и с отдельной логикой для 1x1 + 3x3.Созадл 4 модели как в задании, также обучил их на 15 эпохах на Cifar10.
![2.1_Losses&Accs](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/2.1_ACCS.png)
### Из графиков видно, что лучшая модель с kernel_size 5x5 с accuracy 0.8455(обучалась она медленнее 3x3, но быстрее 7x7, что ожидаемо)
### Визуализируем активации первого слоя
![2.1_activations_1](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/2.1_activations_1.png)
### Вот так модель выделяет зоны на изображении, выделяет зоны.
### 2.2 Влияние глубины CNN (15 баллов)
```python
# Исследуйте влияние глубины CNN:
# - Неглубокая CNN (2 conv слоя)
# - Средняя CNN (4 conv слоя)
# - Глубокая CNN (6+ conv слоев)
# - CNN с Residual связями
# 
# Для каждого варианта:
# - Сравните точность и время обучения
# - Проанализируйте vanishing/exploding gradients
# - Исследуйте эффективность Residual связей
# - Визуализируйте feature maps
```
### Создал классы для каждой модели по заданию,для DeepResNet использовал Residual блок 2 (с Batchnorm и dropout),созданный ранее.
### Получил такие результаты для этого сравнения
1. Model: SmallCNN | Accuracy: 0.6785 | Time: 261.4s
2. Model: MediumCNN | Accuracy: 0.7822 | Time: 319.5s
3. Model: DeepCNN | Accuracy: 0.7423 | Time: 303.5s
4. Model: DeepResNet | Accuracy: 0.8197 | Time: 352.2s
### Самый лучший результат среди этих модлей показал DeepRESNET, доказывая эффективность residual слоев, регуляризации и определенной глубины в контексте Cifar.
### Написал функцию для анализа vanishing/exploding gradients, но не увидел ни тех, ни других в этих моделях (сильного затухания или возрастания на разных слоях)
### Визуалирировал feature maps для каждой из этих моделей для 3 изображений для 8 каналов.
![2.2_feature_maps_1](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/2.2_feature_maps_1.png)
![2.2_feature_maps_2](https://github.com/MakarRybkin/Summer_practice_Deep_Learning/raw/master/plots_for_reports/2.2_feature_maps_2.png)
### Как можно увидеть модели выделяют схожие участки на картинках
### 3.1 Реализация кастомных слоев (15 баллов)
```python
# Реализуйте кастомные слои:
# - Кастомный сверточный слой с дополнительной логикой
# - Attention механизм для CNN
# - Кастомная функция активации
# - Кастомный pooling слой
# 
# Для каждого слоя:
# - Реализуйте forward и backward проходы
# - Добавьте параметры если необходимо
# - Протестируйте на простых примерах
# - Сравните с стандартными аналогами
```
### Написал классы для каждого из 4 кастомных слоев и классы для использующих их моделей:
1. Кастомный сверточный слой (масштабирование результата свертки)
2. Attention механизм для CNN "Squeeze-and-Excitation Networks" (Hu et al., 2018)
3. Кастомная функция активации. Гибрид Swish и Leaky Relu
4. Кастомный pooling слой
### Также написал класс без кастомных слоев для сравнения с кастомнынми.
### Обучал также на Cifar10 на 10 эпохах(из-за этого accuracy сильно ниже, чем у предыдущих моделей) и получил :
1. Model: StandardCNN | Best Accuracy: 0.6984 | Time: 146.8s 
2. Model: CustomConvCNN | Best Accuracy: 0.7170 | Time: 133.8s
3. Model: SE_CNN | Best Accuracy: 0.6860 | Time: 141.6s
4. Model: CustomActivationCNN | Best Accuracy: 0.7030 | Time: 140.2s
5. Model: LpPoolingCNN | Best Accuracy: 0.7140 | Time: 162.4s
### Оказалось, что все кастомные слои кроме модели с Attention механизмом для CNN "Squeeze-and-Excitation Networks" немного лучше, чем стандартная модель. Разница незначительная и скорее всего не является доказательством что эти модели лучше, чем стандартная, а вызвана рандомным выбором весов в начале обучения. Как я уже писал выше придумать хорошие кастомные слои сложно и нужно много времени на подбор параметров для кастомных слоев.
### 3.2 Эксперименты с Residual блоками (15 баллов)¶
```python

# Исследуйте различные варианты Residual блоков:
# - Базовый Residual блок
# - Bottleneck Residual блок
# - Wide Residual блок
# 
# Для каждого варианта:
# - Реализуйте блок с нуля
# - Сравните производительность
# - Проанализируйте количество параметров
# - Исследуйте стабильность обучения
```
### Также создал 3 модели по заданию (самые глубокие из всех рассмотренных) обучил на 8 эпохах(потому что долго обучаются) на CIFAR и получил результаты
1. Model: CNNWithRegularizedResidualst | Best Accuracy: 0.8060 | Time: 182.1s | Parameters: 283,690
2. Model: BottleneckCNN | Best Accuracy: 0.8200 | Time: 814.6s | Model Parameters: 9,216,330
3. Model: WideCNN_16_4 | Best Accuracy: 0.8500 | Time: 351.6s | Model Parameters: 2,749,786
### Последние две модели имеют сильно больше параметров и обучаются дольше. Среди этих моделей самой лучшей оказалась WideCNN с глубиной 16 и шириной 4 с accuracy = 0.85(лучший результат в принципе), BottleneckCNN дольше всех обучается и имеет больше параметров. BottleNeck обучается нестабильно ( accuracy прыгает)

## Выводы :
1. Сверточные сети с residual-блоками сильно превосходят полносвязные на относительно сложных данных (CIFAR-10) — они дают выше точность и лучше обобщаются.

2. Residual-связи позволяют строить более глубокие модели без проблем с градиентами, что повышает стабильность обучения.

3. Размер ядра и глубина важны: оптимальными оказались 5×5 ядра и глубина с residual-блоками (например, DeepResNet), которые показали лучший баланс скорости и качества.

4. WideResNet дал лучший результат (accuracy = 0.85 всего на 8 эпохах), но требует больше параметров и времени на обучение.

5. Кастомные слои могут незначительно улучшать качество, но их польза не всегда очевидна и зависит от задачи.
